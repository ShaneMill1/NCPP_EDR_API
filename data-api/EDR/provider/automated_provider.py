import datetime
from datacube.utils.cog import write_cog
import xarray as xr
import geopandas as gpd
import copy
from EDR.provider import concatenate_covjson
import glob
import json
from shapely.geometry import Point, Polygon, LineString, MultiLineString
from shapely.ops import transform
from EDR.templates import edrpoint, edrpolygon
import pyproj
import numpy as np
import os
from EDR.provider.base import BaseProvider, ProviderConnectionError, ProviderQueryError
from EDR.provider import InvalidProviderError
from EDR.formatters import format_output as fo
import yaml
import dask
import rioxarray
from EDR.provider import convert_to_grib
import flask
import urllib.request, json
import uuid as ud
import re
import cfgrib
import zipfile
import pathlib
import s3fs

class AutomatedCollectionProvider(BaseProvider):
    def __init__(self,dataset,config):
        """initializer"""
        dsn=dataset.split('_')
        ds_name=dsn[0]+'_'+dsn[1]
        self.DATASET_FOLDER = config['datasets'][ds_name]['provider']['data_source']
        self.dir_root=self.DATASET_FOLDER.replace('/collections','')
        self.fkey='step'
        self.ftime_key='valid_time'

    def load_data(self,cid,instance,model):
       fs=s3fs.S3FileSystem()
       zarr_loc=self.DATASET_FOLDER+'/'+model+'/'+instance+'/'+cid
       zarr_obj=xr.open_zarr(zarr_loc)
       return zarr_obj
    
   
    def load_collection_meta(self,ds,cid):
       for dv in ds.data_vars:
          first_dv=dv
          break
       self.parameters=ds.data_vars
       self.dimensions=ds.dims
       self.level_type=ds[first_dv].GRIB_typeOfLevel
       self.lvkey=self.level_type
       return
    
    def find_initial_time(self,initial_time):
       initial_time=initial_time.replace('(','')
       initial_time=initial_time.replace(')','')
       initial_time=initial_time.replace('/','-')
       month=initial_time[0:2]
       day=initial_time[3:5]
       year=initial_time[6:10]
       time=initial_time[11:16]
       initial_time=year+'-'+month+'-'+day+' '+time+':00'
       initial_time=datetime.datetime.strptime(initial_time, "%Y-%m-%d %H:%M:%S")
       return initial_time
 
    def pt_to_covjson(self,query_dict,coords,qtype):
       
       output=query_dict
       output['domain']={}
       output['type']="Coverage"
       output['domain']['type']="Domain"
       output['parameters'] = output.pop('data_vars')
       output['domain']['axes']={}
       if qtype=='point':
          output['domain']['domainType']="PointSeries"
          output['domain']['axes']['x']={}
          output['domain']['axes']['y']={}
          output['domain']['axes']['x']['values']=[output['coords']['longitude']['data']]
          output['domain']['axes']['y']['values']=[output['coords']['latitude']['data']]
       elif qtype=='polygon':
          output['domain']['domainType']="Grid"
          output['domain']['axes']['x']={}
          output['domain']['axes']['y']={}
          output['domain']['axes']['y']['values']=output['coords']['latitude']['data']
          output['domain']['axes']['x']['values']=output['coords']['longitude']['data']
       elif qtype=='linestring' or 'linestringm' or 'linestringz' or 'linestringzm':
          output['domain']['axes']['composite']={}
          output['domain']['domainType']="Trajectory"
          output['domain']['axes']['composite']={}
          output['domain']['axes']['composite']['coordinates']=["t","x","y","z"]
          #Values is a list containing t, x, y, z in that order.
          #Need to tie this to the dictionary generated by xarray
          output['domain']['axes']['composite']['values']=[]
       del output['coords']['latitude']['data']
       del output['coords']['longitude']['data']
       output['ranges']={}
       #find dim with "lv" and create the "z" dimension
       for key in output['coords']:
          if key != 'latitude' and key != 'longitude' and key != 'step' and 'time' not in key and key !='spatial_ref':
             z_key=key
             output['domain']['axes']['z']={}
             if type(output['coords'][z_key]['data']) != list:
                try:
                   data_val=float(output['coords'][z_key]['data'])
                except:
                   data_val=output['coords'][z_key]['data']
                output['domain']['axes']['z']['values']=[data_val] 
             else:
                data_val=list()
                for d in output['coords'][z_key]['data']:
                   try:
                      data_val.append(float(d))
                   except:
                      data_val=output['coords'][z_key]['data']
                output['domain']['axes']['z']['values']=data_val 
       for p in output['parameters']:
          output['ranges'][p]={}
          output['ranges'][p]['values']=np.array(output['parameters'][p]['data']).flatten().tolist()
          output['ranges'][p]['type']='NdArray'
          output['ranges'][p]['dataType']='float'
          output['parameters'][p]['description']={}
          output['parameters'][p]['description']['en']=output['parameters'][p]['attrs']['long_name']
          output['parameters'][p]['unit']={}
          output['parameters'][p]['unit']['symbol']={}
          output['parameters'][p]['unit']['symbol']['value']=output['parameters'][p]['attrs']['units']
          output['parameters'][p]['unit']['symbol']['type']=''
          output['parameters'][p]['unit']['label']={}
          output['parameters'][p]['unit']['label']['en']=output['parameters'][p]['attrs']['long_name']
          output['parameters'][p]['observedProperty']={}
          output['parameters'][p]['observedProperty']['label']={}
          output['parameters'][p]['observedProperty']['label']['en']=output['parameters'][p]['attrs']['long_name']
          del output['parameters'][p]['data']
          del output['parameters'][p]['dims']
       #Create iso dates from metadata and insert as t dimension for covjson
          if isinstance(output['coords'][self.ftime_key]['data'],list):
             count_for_time=len(output['ranges'][p]['values'])
             f_timedelta=output['coords'][self.ftime_key]['data']
             iso_time=[]
             for t in f_timedelta:
                time=t.isoformat()
                iso_time.append(time)
             output['domain']['axes']['t']={}
             output['domain']['axes']['t']['values']=iso_time
          else:
             iso_time=[output['coords'][self.ftime_key]['data']]
             count_for_time=len([output['ranges'][p]['values']])
             f_timedelta=[output['coords'][self.ftime_key]['data']]
             iso_time=[]
             for t in f_timedelta:
                time=t.isoformat()
                iso_time.append(time)
             output['domain']['axes']['t']={}
             output['domain']['axes']['t']['values']=iso_time
       a=list()
       output['domain']["referencing"]=[]
       #need to set output['domain']['axes']['composite']['values']=[] outside of loop
       #once iso 8601 time is determined
       if qtype=='linestring' or qtype=='linestringm' or qtype=='linestringz' or qtype=='linestringzm':
          for idc,cr in enumerate(coords):
             cr_list=list()
             if len(iso_time)==1:
                #case where the iso time is fixed
                cr_list.append(iso_time[0])
                cr_list.append(cr[1])
                cr_list.append(cr[0])
                if qtype=='linestringz':
                   cr_list.append(cr[2])     
             if len(iso_time)>1:
                #case where iso time is not fixed
                if len(cr)==3:
                   cr_list.append(iso_time[idc])
                   cr_list.append(float(cr[1]))
                   cr_list.append(float(cr[0]))
                if len(cr)==4:
                   cr_list.append(iso_time[idc])
                   cr_list.append(float(cr[1]))
                   cr_list.append(float(cr[0]))
                   try:
                      cr_list.append(float(cr[3]))
                   except:
                      cr_list.append(float(cr[2]))
             try:
                if len(output['domain']['axes']['z']['values'])==1:
                   cr_list.append(output['domain']['axes']['z']['values'][0])
             except:
                print('this collection does not contain levels')
             output['domain']['axes']['composite']['values'].append(cr_list)
          output['domain']['axes']['composite']['dataType']="tuple"
       for p in output['parameters']:
          key_dir={}
          for k in list(output['domain']['axes'].keys()):
             if k != 'x' and k != 'y':
                if k not in a:
                   a.append(k)
             #Order matters, for polygon need t,y,x
             if k == 't':
                output['domain']["referencing"].append({"coordinates": ["t"],"system": {"type": "TemporalRS","calendar":"Gregorian"}})
             if k == 'z':
                output['domain']["referencing"].append({"coordinates": ["z"],"system": {"type": 'z info'}})
             if k == 'x':
                output['domain']["referencing"].append({"coordinates":["x","y"],"system":{"type": "GeographicCRS","id": "http://www.opengis.net/def/crs/OGC/1.3/CRS84"}})
             try:
                key_dir[k]=len(output['domain']['axes'][k]['values'])
             except:
                key_dir[k]=len([output['domain']['axes'][k]['values']])
          a=sorted(a)
          if 'composite' not in key_dir:
             if 'x' in key_dir and 'y' in key_dir and 'z' in key_dir and 't' in key_dir:
                output['ranges'][p]['axisNames']=['t','z','y','x']
                output['ranges'][p]['shape']=[key_dir['t'],key_dir['z'],key_dir['y'],key_dir['x']]
             if 'x' in key_dir and 'y' in key_dir and 'z' not in key_dir and 't' in key_dir:
                output['ranges'][p]['axisNames']=['t','y','x']
                output['ranges'][p]['shape']=[key_dir['t'],key_dir['y'],key_dir['x']]
             if 'x' in key_dir and 'y' in key_dir and 'z' in key_dir and 't' not in key_dir:
                output['ranges'][p]['axisNames']=['z','y','x']
                output['ranges'][p]['shape']=[key_dir['z'],key_dir['y'],key_dir['x']]
             if 'x' in key_dir and 'y' in key_dir and 'z' not in key_dir and 't' not in key_dir:
                output['ranges'][p]['axisNames']=['y','x']
                output['ranges'][p]['shape']=[key_dir['y'],key_dir['x']]
          if 'composite' in key_dir:
             output['ranges'][p]['axisNames']=['composite']
             if isinstance(output['ranges'][p]['values'][0], list):
                flat_list = [item for sublist in output['ranges'][p]['values'] for item in sublist]
             else:
                flat_list = output['ranges'][p]['values']
             output['ranges'][p]['values']=flat_list
             output['ranges'][p]['shape']=[len(output['ranges'][p]['values'])]
             try:
                del output['domain']['axes']['t']
             except:
                print('this query does not vary along the time dimension')
             try:
                del output['domain']['axes']['z']
             except:
                print('this collection does not contain a zedd level')
       #add referencing
       #need to take care of this by appending to list based on coordinates available:
       output['domain']["referencing"]=list()
       output['domain']["referencing"]=[{"coordinates":["y","x"],"system":{"type": "GeographicCRS","id": "http://www.opengis.net/def/crs/OGC/1.3/CRS84"}},{"coordinates": ["t"],"system": {"type": "TemporalRS","calendar":"Gregorian"}}]
       del output['coords']       
       del output['attrs']
       del output['dims']
       return output


    def query(self,dataset, qtype, coords, time_range, z_value, params, instance, outputFormat):
       #using the collection meta, make query based on the dimensions, so the automated process would be to formulate kwargs from the dimension list
       #important coords is an array of [lon,lat]
       #correct lon to fit 0->360
       qtype_endpoint=flask.request.url.split('?')[0].split('/')[-1]
       self.uuid=str(ud.uuid4().hex)
       self.lv_list=list();self.param_list=list()
       cid="_".join(dataset.split("_", 2)[1:])
       model=dataset.split("_")[1]+'_'+dataset.split("_")[2]
       ds=self.load_data(cid,instance,model)
       self.load_collection_meta(ds,cid)
       dim_key=self.dimensions
       if len(params)>0:
           output=ds[params]
       else:
           output=ds
       if '/' in z_value:
           z_value=np.array(z_value.split('/')).astype(np.float64).tolist()
           output=output.sel({self.lvkey: slice(z_value[0],z_value[1])})
       else:
           output=output.sel({self.lvkey: z_value})
       #output=output.sel({self.fkey: time_range})
       if qtype=='point':
          output, output_boolean=get_point_data(self,dataset, qtype, coords, time_range, z_value, params, instance, outputFormat, output)
          return output, output_boolean
       if qtype=='polygon':
          output, output_boolean=get_polygon_data(self,dataset, qtype, coords, time_range, z_value, params, instance, outputFormat, output)
          return output, output_boolean
       if qtype_endpoint=='trajectory':
          output, output_boolean=get_trajectory_data(self, dataset, qtype, coords, time_range, z_value, params, instance, outputFormat, output)
          return output, output_boolean
       if qtype_endpoint=='corridor':
          output, output_boolean=get_corridor_data(self, dataset, qtype, coords, time_range, z_value, params, instance, outputFormat, output)
          return output, output_boolean



#Sampling Geometry Type Modules
#Eventually, even these modules can be separated out more to be included in the conversion modules
def get_point_data(self,dataset, qtype, coords, time_range, z_value, params, instance, outputFormat, output):
   query_args={'latitude': coords[1], 'longitude': coords[0]}
   lonattr='longitude'
   output=output.assign_coords({'longitude': getattr(output,'longitude') - 360})
   output=output.rio.set_spatial_dims('longitude','longitude',inplace=True)
   output=output.rio.write_crs(4326)
   output=output.sel(query_args, method='nearest')
   output=output.to_dict()
   if outputFormat=="CoverageJSON": #this if statement eventually will only occur once for all geom types
      output=self.pt_to_covjson(output,coords,qtype)
   return json.dumps(output, indent=4, sort_keys=True, default=str).replace('NaN','null'), 'no_delete'




def get_polygon_data(self,dataset, qtype, coords, time_range, z_value, params, instance, outputFormat, output):
          geometries=[];coord_list=list()
          #a new coords list is needed in order for rio.clip to correctly extract the polygon. It had a false offset of 1 in the wkt. Need to apply this only for bbox's
          #so this will occur when bbox wkt length is 5
          if len(coords) == 5:
             coords_clip=[[coords[0][0],coords[0][1]],[coords[1][0],coords[1][1]],[coords[2][0]-1,coords[2][1]],[coords[3][0]-1,coords[3][1]],[coords[4][0],coords[4][1]]]
          else:
             coords_clip=coords
          #---------------------
          geometries.append({'type':'Polygon', 'coordinates':[coords_clip]})
          output_adl=output
          output=output.assign_coords({'longitude': getattr(output,'longitude') - 360})
          output_bdl=output
          output=xr.concat([output_bdl,output_adl],dim='lon_0')
          output=output.rio.set_spatial_dims('longitude','latitude',inplace=True)
          output=output.rio.write_crs(4326)
          output=output.rio.clip(geometries,output.rio.crs)
          if outputFormat=="CoverageJSON":
             output=output.to_dict()
             output=self.pt_to_covjson(output,coords,qtype)
             return json.dumps(output, indent=4, sort_keys=True, default=str).replace('NaN','null'), 'no_delete'




def get_trajectory_data(self,dataset, qtype, coords, time_range, z_value, params, instance, outputFormat, output):
       initial_time=output[params[0]].initial_time
       initial_time=self.find_initial_time(initial_time)
       if qtype=='linestringm' or qtype=='linestring':
          if z_value:
             output=output.sel({self.lvkey:z_value})
       if qtype == 'linestring':
          lat_list=list();lon_list=list()
          for m in coords:
             lat=float(m[1])
             lon=float(m[0])+360
             lat_list.append(lat)
             lon_list.append(lon)
          query_args={'latitude': xr.DataArray(lat_list), 'longitude': xr.DataArray(lon_list)}
          output=output.sel(query_args, method='nearest')
          if outputFormat=="CoverageJSON":
             output=output.to_dict()
             output=self.pt_to_covjson(output,coords,qtype)
             return json.dumps(output, indent=4, sort_keys=True, default=str).replace('NaN','null'), 'no_delete'
       if qtype == 'linestringm' or qtype == 'linestringz' or qtype == 'linestringzm':
          lat_list=list();lon_list=list();time_list=list();zedd_list=list()
          for m in coords:
              lat=float(m[1])
              lon=float(m[0])+360
              if qtype=='linestringm':
                 time=m[2]
                 lat_list.append(lat)
                 lon_list.append(lon)
                 time=datetime.datetime.strptime(str(time), "%Y-%m-%dT%H:%M:%S")-initial_time
                 time_list.append(time)
              if qtype=='linestringz':
                 zedd=m[2]
                 lat_list.append(lat)
                 lon_list.append(lon)
                 zedd_list.append(zedd)
              if qtype=='linestringzm':
                 zedd=m[2]
                 time=m[3]
                 time=datetime.datetime.strptime(str(time), "%Y-%m-%dT%H:%M:%S")-initial_time
                 time_list.append(time)
                 lat_list.append(lat)
                 lon_list.append(lon)
                 zedd_list.append(zedd)
          if qtype=='linestringm':
             query_args={'latitude': xr.DataArray(lat_list), 'longitude': xr.DataArray(lon_list), self.fkey: xr.DataArray(time_list)}
          if qtype=='linestringz':
             query_args={'latitude': xr.DataArray(lat_list), 'longitude': xr.DataArray(lon_list), self.lvkey: xr.DataArray(zedd_list)}
          if qtype=='linestringzm':
             query_args={'latitude': xr.DataArray(lat_list), 'longitude': xr.DataArray(lon_list), self.lvkey: xr.DataArray(zedd_list), self.fkey: xr.DataArray(time_list)}
          output=output.sel(query_args, method='nearest')
          if outputFormat=="CoverageJSON":
             output=output.to_dict()
             output=self.pt_to_covjson(output,coords,qtype)
             return json.dumps(output, indent=4, sort_keys=True, default=str).replace('NaN','null'), 'no_delete'



def get_angles(vec_1,vec_2):
    """
    return the angle, in degrees, between two vectors
    """
    
    dot = np.dot(vec_1, vec_2)
    det = np.cross(vec_1,vec_2)
    angle_in_rad = np.arctan2(det,dot)
    return np.degrees(angle_in_rad)



def simplify_by_angle(poly_in, deg_ratio_tol = 0.05):
    """
    try to remove persistent coordinate points that remain after
    simplify, convex hull, or something, etc. with some trig instead
    params:
    poly_in: shapely Polygon 
    deg_tol: degree tolerance for comparison between successive vectors
    return: 'simplified' polygon
    
    """
    
    ext_poly_coords = poly_in.coords[:]
    vector_rep = np.diff(ext_poly_coords,axis = 0)
    angles_list = []
    angle_ratio_list=[]
    for i in range(0,len(vector_rep) -1 ):
        angles_list.append(np.abs(get_angles(vector_rep[i],vector_rep[i+1])))
    max_of_list=max(angles_list)
    for a in angles_list:
       ratio=a/max_of_list
       angle_ratio_list.append(ratio)
    index_remove_list=list()
    for ida,a in enumerate(angle_ratio_list,start=1):
       if a < deg_ratio_tol:
          index_remove_list.append(ida)
    return index_remove_list



def get_corridor_data(self,dataset, qtype, coords, time_range, z_value, params, instance, outputFormat, output):
             model_resolution=float(dataset.split('_')[2])
             try:
                resolution_x=flask.request.args['resolution-x']
             except:
                resolution_x=None
             try:
                resolution_y=int(flask.request.args['resolution-y'])
             except:
                resolution_y=None
             try:
                resolution_z=flask.request.args['resolution-z']
             except:
                resolution_z=None
             try:
                corridor_height=flask.request.args['corridor-height']
             except:
                corridor_height=None
             if resolution_y == '0':
                resolution_y_result=float(model_resolution)/100
             if resolution_y==None:
                resolution_y_result=None
             initial_time=output[params[0]].initial_time
             initial_time=self.find_initial_time(initial_time)
             #Need to transform from 4326 to web mercator (3857)
             transformer_1 = pyproj.Transformer.from_proj(pyproj.Proj(init='epsg:4326'), pyproj.Proj(init='epsg:3857'))
             transformer_2 = pyproj.Transformer.from_proj(pyproj.Proj(init='epsg:3857'), pyproj.Proj(init='epsg:4326'))
             try:
                #km to meters
                corridor_width_arg=float(flask.request.args['corridor-width'])*1000
             except:
                #default is 100 km
                corridor_width_arg=100000
             #join style had been 2, setting to default for now
             list_4326=list();list_3857=list()
             #need to make the list index dynamic to fill the width of the corridor. This can be x amount of total trajectories that will be aggregated to populate the covjson coverage
             #collection output.
             list_index=['left','center','right']
             coords_xy=list()
             for cxy in coords:
                cxy_new=[float(cxy[0]),float(cxy[1])]
                coords_xy.append(cxy_new)
             coords_3857 = transform(transformer_1.transform, LineString(coords_xy))
             for li in list_index:
                if li=='left':
                   c_og=coords_3857.parallel_offset(corridor_width_arg/2, 'left',join_style=2).coords[:]
                   c_og_3857=list(c_og)
                   #simplify for testing:
                   c_og_4326=list(transform(transformer_2.transform, LineString(c_og)).coords)
                if li=='center':
                   c_og=transform(transformer_1.transform, LineString(coords_xy))
                   #c_og_3857=list(c_og)
                   #simplify for testing:
                   c_og_4326=list(transform(transformer_2.transform, LineString(c_og)).coords)
                if li=='right':
                   c_og=coords_3857.parallel_offset(corridor_width_arg/2, 'right',join_style=2).coords[:]
                   c_og_3857=list(c_og)
                   c_og_3857.reverse()
                   #simplify for testing:
                   c_og_4326=list(transform(transformer_2.transform, LineString(c_og)).coords)
                   c_og_4326.reverse()
                list_4326.append(c_og_4326);list_3857.append(c_og_3857)
             #left outside
             left_outside=list_4326[0]
             #right outside
             right_outside=list_4326[2]
             list_4326_new=list();traj_list=list()
             diff=abs(right_outside[0][0]-left_outside[0][0])
             lo_3857=list_3857[0]
             ro_3857=list_3857[2]
             diff_3857=float(corridor_width_arg)
             if resolution_x is not None and resolution_x != '0':
                x_res_step=diff_3857/(float(resolution_x)-1)
                x_res_boolean=True
             else:
                #for now, approx to based on 111000 to 1.0 assumption
                one_degree=111000.0
                x_res_step=one_degree*model_resolution
                x_res_boolean=False
             if diff>1 or x_res_boolean==True:
                #populate list
                list_4326_new.append(left_outside)
                tracker=list_4326_new[-1][0][0]
                while tracker<right_outside[0][0]:
                   #loop for each lat/lon pair
                   for t in list_4326_new:
                      t_3857=transform(transformer_1.transform, LineString(t))
                      t_og=t_3857.parallel_offset(x_res_step,'right',resolution=0,join_style=2).coords[:]
                      t_og=list(transform(transformer_2.transform, LineString(t_og)).coords)
                      t_og.reverse()
                   list_4326_new.append(t_og)
                   tracker=list_4326_new[-1][0][0]
                list_4326=list_4326_new
             result=list()
             for idl,ls_el in enumerate(list_4326):
                points_to_remove=simplify_by_angle(LineString(ls_el))
                for index in sorted(points_to_remove, reverse=True):
                   del ls_el[index]
                result.append(ls_el)
             list_4326=result
             output_list=list();
             if 'linestring' in qtype:
                for waypoints in list_4326:
                   lat_list=list();lon_list=list();time_list=list();zedd_list=list();coords_list=list();seg_dist_list=list()
                   for idm,m in enumerate(waypoints):
                      lat=float(m[1])
                      lon=float(m[0])
                      if qtype=='linestringm':
                         time=coords[idm][2]
                         time=datetime.datetime.strptime(str(time), "%Y-%m-%dT%H:%M:%S") - initial_time
                         time_list.append(time)
                      if qtype=='linestringz':
                         zedd=coords[idm][2]
                         zedd_list.append(zedd)
                      if qtype=='linestringzm':
                         zedd=coords[idm][2]
                         time=coords[idm][3]
                         time=datetime.datetime.strptime(str(time), "%Y-%m-%dT%H:%M:%S") - initial_time
                         time_list.append(time)
                         zedd_list.append(zedd)
                   #if resolution_y == 0:
                   for idm,m in enumerate(waypoints):
                      lat=float(m[1])
                      lon=float(m[0])
                      lat_list.append(lat)
                      lon_list.append(lon)
                   gdf = gpd.GeoDataFrame(
                      geometry=[LineString(waypoints)],
                   )
                   lon_vert=gdf.values[0][0].xy[0].tolist()
                   lat_vert=gdf.values[0][0].xy[1].tolist()
                   seg_list=list()
                   lon_list=list();lat_list=list()
                   new_seg_length=0
                   for idl,l in enumerate(lon_vert):
                      try:
                         lon_i=lon_vert[idl]
                         lat_i=lat_vert[idl]
                         lon_f=lon_vert[idl+1]
                         lat_f=lat_vert[idl+1]
                         seg=LineString([(lon_i,lat_i),(lon_f,lat_f)])
                         new_seg_length=new_seg_length+seg.length
                         seg_dist_list.append(new_seg_length)
                         point_list=list()
                         if resolution_y ==0:
                            distance=0;add_distance=resolution_y_result/10
                            while distance < seg.length:
                               new_point = seg.interpolate(distance).coords[:][0]
                               lon_list.append(new_point[0])
                               lat_list.append(new_point[1])
                               distance += add_distance
                      except:
                         pass
                   if resolution_y > 0:
                      distance = 0 
                      l_3857=transform(transformer_1.transform, LineString(waypoints))
                      line=LineString(l_3857)
                      add_distance=line.length/float(resolution_y)
                      point_list=list()
                      while distance < line.length:
                         new_point = line.interpolate(distance)
                         point_list.append(new_point)
                         distance += add_distance
                      point_4326=list(transform(transformer_2.transform, LineString(point_list)).coords)
                      for p in point_4326:
                         lon_list.append(p[0])
                         lat_list.append(p[1])
                         coords_list.append((p[0],p[1]))
                      query_args={'latitude': xr.DataArray(lat_list), 'longitude': xr.DataArray(lon_list)}
                   else:
                      point_4326=waypoints
                      lat_list=lat_vert;lon_list=lon_vert
                      if len(time_list)>0 or len(zedd_list)>0:
                         if len(time_list) > 0:
                            time_interp_list=time_list
                         if len(zedd_list) > 0:
                            zedd_interp_list=zedd_list
                         for idx,l in enumerate(lat_list):
                            if len(time_list)>0 and len(zedd_list)==0:
                               coords_list.append((lon_list[idx],lat_list[idx],time_interp_list[idx]))
                            if len(zedd_list)>0 and len(time_list)==0:
                               coords_list.append((lon_list[idx],lat_list[idx],zedd_interp_list[idx]))
                            if len(zedd_list)>0 and len(time_list)>0:
                               coords_list.append((lon_list[idx],lat_list[idx],time_interp_list[idx],zedd_interp_list[idx]))
                      else:
                         for idx,l in enumerate(lat_list):
                            coords_list.append((lon_list[idx],lat_list[idx]))
                         query_args={'latitude': xr.DataArray(lat_list), 'longitude': xr.DataArray(lon_list)}
                   if (len(time_list) > 0 or len(zedd_list) > 0) and resolution_y > 0:
                      waypoint_linestring=LineString(waypoints)
                      wp_dist_list=list()
                      for idw, w in enumerate(point_4326):
                         try:
                            distance_between_p=round(LineString([point_4326[idw],point_4326[idw+1]]).length,3)
                            wp_dist_list.append(distance_between_p)
                         except:
                            pass
                      wp_counter=0;wp_idx=list();overall_index_list=list();wp_idx_counter=0
                      #overall_index_list is not being populated correctly and needs to be rewritten
                      #what is needed? start (0), stop (seg_dist), step (wp_dist)
                      #point_list is a list of all of the point objects
                      #wp_dist_list is being calculated by shapely as the distance between each point. this seems to be the core of the issue and I need to find another approach.
                      #seg_dist_list is the total distance between waypoints which was serving as the end value for the while statement/looping
                      for widx,wp_dist in enumerate(wp_dist_list):
                         for ids,seg_dist in enumerate(seg_dist_list):
                            seg_index_list=list()
                            while wp_counter <= seg_dist:
                               if wp_counter <= seg_dist:
                                  print('wp_counter: '+str(wp_counter)+ ' | seg_dist: '+str(seg_dist))
                                  seg_index_list.append(wp_idx_counter)
                                  wp_counter=wp_counter+wp_dist
                                  wp_idx_counter=wp_idx_counter+1
                            if len(seg_index_list) > 1:
                               overall_index_list.append(seg_index_list)
                      time_interp_list=list();zedd_interp_list=list()                      
                      for ids,seg in enumerate(overall_index_list):
                         seg_len=len(seg)
                         if len(time_list)>0:
                            time_interp=time_list[ids]
                            time_diff=time_list[ids+1]-time_list[ids]
                            time_step=time_diff/seg_len
                            time_interp_list=np.arange(time_interp,time_list[ids+1],time_step).tolist()
                         if len(zedd_list)>0:
                            zedd_interp=zedd_list[ids]
                            zedd_diff=float(zedd_list[ids+1])-float(zedd_list[ids])
                            zedd_step=zedd_diff/seg_len
                            zedd_interp_list=np.arange(float(zedd_interp),float(zedd_list[ids+1]),zedd_step).tolist()
                      for idx,l in enumerate(lat_list):
                         if len(time_list)>0 and len(zedd_list)==0:
                            coords_list.append((lon_list[idx],lat_list[idx],time_interp_list[idx]))
                         if len(zedd_list)>0 and len(time_list)==0:
                            coords_list.append((lon_list[idx],lat_list[idx],zedd_interp_list[idx]))
                         if len(zedd_list)>0 and len(time_list)>0:
                            coords_list.append((lon_list[idx],lat_list[idx],time_interp_list[idx],zedd_interp_list[idx]))
                      time_dt_list=list()
                      try:
                         for time_iso in output[self.fkey].values:
                            time_dt=datetime.datetime.strptime(str(time_iso), "%Y-%m-%dT%H:%M:%S")
                            time_delta=time_dt-initial_time
                            time_dt_list.append(time_delta)
                         output=output.assign_coords({self.fkey:time_dt_list})
                      except:
                         #conversion to delta already occurred
                         pass
                      for idx,l in enumerate(lat_list):
                         query_args={'latitude': xr.DataArray(lat_list), 'longitude': xr.DataArray(lon_list)}
                   if qtype=='linestringm':
                      query_args={'latitude': xr.DataArray(lat_list), 'longitude': xr.DataArray(lon_list), self.fkey: xr.DataArray(time_interp_list)}
                   if qtype=='linestringz':
                      query_args={'latitude': xr.DataArray(lat_list), 'longitude': xr.DataArray(lon_list), self.lvkey: xr.DataArray(zedd_interp_list)}
                   if qtype=='linestringzm':
                      query_args={'latitude': xr.DataArray(lat_list), 'longitude': xr.DataArray(lon_list), self.lvkey: xr.DataArray(zedd_interp_list), self.fkey: xr.DataArray(time_interp_list)}
                   if corridor_height == None or corridor_height=='0':
                      output_result=output.sel(query_args, method='nearest')
                      if z_value:
                         try:
                            output_result=output_result.sel({self.lvkey:z_value})
                         except:
                            pass
                      output_dict=output_result.to_dict()
                      output_list.append(output_result)
                      output_json=self.pt_to_covjson(output_dict,coords_list,qtype)
                      traj_list.append(output_json)
                   else:
                      try:
                         output_result=output
                         if qtype=='linestring' or qtype=='linestringm':
                            lv_index=np.where(output_result[self.lvkey].values==float(z_value))[0][0]
                            lv_first=lv_index-int(corridor_height)
                            lv_last=lv_index+int(corridor_height)+1
                            lv_list=output_result[self.lvkey].values[lv_first:lv_last].tolist()
                            for lv in lv_list:
                               output_lv=output_result.sel({self.lvkey:lv},method='nearest')
                               output_lv=output_lv.sel(query_args,method='nearest')
                               output_dict=output_lv.to_dict()
                               output_list.append(output_lv)
                               output_json=self.pt_to_covjson(output_dict,coords_list,qtype)
                               traj_list.append(output_json)
                         elif qtype=='linestringz' or 'linestringzm':
                            output_result=output
                            lv_series=query_args[self.lvkey]
                            all_levels=list()
                            for lvs in lv_series:
                               lvs_val=float(lvs.values.tolist())
                               lv_index=np.where(output_result[self.lvkey].values==lvs_val)[0][0]
                               lv_first=lv_index-int(corridor_height)
                               lv_last=lv_index+int(corridor_height)+1
                               lv_list=output_result[self.lvkey].values[lv_first:lv_last].tolist()
                               all_levels.append(lv_list)
                            query_levels=np.vstack(np.array(all_levels)).T.tolist()
                            for qlv in query_levels:
                               query_args_sel=query_args
                               query_args_sel[self.lvkey]=xr.DataArray(qlv)
                               output_qlv=output_result.sel(query_args_sel, method='nearest')
                               output_dict=output_qlv.to_dict()
                               output_list.append(output_qlv)
                               new_coords_list=list()
                               for idc,cl in enumerate(coords_list):
                                  new_cl=cl[0:-1]+(qlv[idc],)   
                                  new_coords_list.append(new_cl)
                               output_json=self.pt_to_covjson(output_dict,new_coords_list,qtype)
                               traj_list.append(output_json)
                      except:
                         print('given the corridor height argument, corridor height extended beyond the bounds available')
                cov_collection=concatenate_covjson.concat_covjson(traj_list)      
             if outputFormat=="CoverageJSON":
                return json.dumps(cov_collection, indent=4, sort_keys=True, default=str).replace('NaN','null'), 'no_delete'



#Conversion modules
def convert_traj_to_cf(self,output,output_dict,initial_time):
             time_dt_list=list()
             if not isinstance(output[self.fkey].values,list):
                t_vals=[output[self.fkey].values]
             else:
                t_vals=output[self.fkey].values
             for time_iso in t_vals: 
                time_delta=str(np.datetime64(initial_time)-time_iso).split('.')[0]             
                time_dt_list.append(time_delta)
             output=output.assign_coords({self.fkey:time_dt_list})
             grib_template=self.DATASET_FOLDER+'/'+self.model+'/'+self.cycle+'/'+self.cycle+'_'+self.model+'_template.grib'
             convert_to_grib.create_grib(output_dict,grib_template,self.lv_list,self.uuid,self.dir_root,False)
             template_list=cfgrib.open_datasets(self.dir_root+'/output-'+self.uuid+'.grb')
             template_data=xr.open_dataset(self.dir_root+'/output-'+self.uuid+'.grb',engine='cfgrib')
             actual_data=output
             for coord_og in output.coords:
                if 'forecast_time' in coord_og:
                   actual_data=actual_data.rename({coord_og: 'step'})
                   initial_time=np.datetime64(self.find_initial_time(actual_data.data_vars[next(iter(actual_data.data_vars))].initial_time))
                   valid_time=actual_data['step'].values 
                   actual_data=actual_data.assign_coords({'valid_time': valid_time})
                   actual_data=actual_data.assign_coords({'time': initial_time})
                for coord_new in template_data.coords:
                   try:
                      if output[coord_og].long_name == template_data[coord_new].long_name:
                         actual_data=actual_data.rename({coord_og: coord_new})
                      print(coord_og+' |break| '+coord_new)
                   except:
                      print('coord passed')
                   if 'lv_' in coord_og:
                      coord_og_split=coord_og.split('_')
                      for c in coord_og_split:
                         if c.lower() in coord_new.lower():
                            try:
                               actual_data=actual_data.rename({coord_og: coord_new})
                               actual_data[coord_new]=actual_data[coord_new].assign_attrs(long_name=template_data[coord_new].long_name)
                               actual_data[coord_new]=actual_data[coord_new].assign_attrs(units=template_data[coord_new].units)
                            except:
                               pass
                      try:
                         if actual_data[coord_og].long_name:
                            if actual_data[coord_og].long_name.lower().split(' ')[0] in coord_new.lower():
                               actual_data=actual_data.rename({coord_og: coord_new})
                               actual_data[coord_new]=actual_data[coord_new].assign_attrs(long_name=template_data[coord_new].long_name)
                               actual_data[coord_new]=actual_data[coord_new].assign_attrs(units=template_data[coord_new].units)
                               break
                      except:
                         pass
             for data_var_og in output.data_vars:
                for t in template_list:
                   for data_var_new in t.data_vars:
                      output_initial=output[data_var_og].long_name.replace('-',' ')
                      output_initial=re.sub(r" ?\([^)]+\)", "", output_initial)
                      print(output_initial.lower()+' |break| '+ t[data_var_new].long_name.lower())
                      if output_initial.lower() == t[data_var_new].long_name.lower():
                         actual_data=actual_data.rename({data_var_og: data_var_new})
             actual_data.attrs=template_data.attrs
             actual_data=actual_data.rename({'dim_0':'trajectory'})
             actual_data=actual_data.assign_coords({'trajectory':actual_data['trajectory']})
             actual_data['trajectory'].attrs={'cf_role':'trajectory_id','long_name':'trajectory','units':'degrees'}
             actual_data.attrs['_FillValue']='nan'
             actual_data=actual_data.assign_attrs({"featureType": "trajectory"})
             return actual_data



def convert_corridor_to_cf(self,output_list):
   actual_data=xr.Dataset()
   for idx,traj_output in enumerate(output_list):
      traj_output=traj_output.rename({'dim_0':'trajectory_'+str(idx)})
      traj_output=traj_output.rename({'lat_0':'lat_0_'+str(idx)})
      traj_output=traj_output.rename({'lon_0':'lon_0_'+str(idx)})
      traj_output['trajectory_'+str(idx)].attrs['cf_role']='trajectory_id'
      for data_vars in traj_output.data_vars:
         traj_output=traj_output.rename({data_vars:data_vars+'_trajectory_'+str(idx)}) 
      actual_data=actual_data.merge(traj_output,compat='override')
      actual_data=actual_data.assign_coords({'trajectory_'+str(idx):actual_data['trajectory_'+str(idx)]})
      actual_data['trajectory_'+str(idx)].attrs={'long_name':'trajectory'+str(idx),'units':'degrees'}
      if idx==1:
         actual_data['trajectory_'+str(idx)].attrs.update({'cf_role': 'trajectory_id'})
   actual_data=actual_data.assign_attrs({"featureType": "trajectory"})
   actual_data.attrs['Conventions']='CF-1.7'
   return actual_data



def export_geotiff(self,output):
   dim_tracker={}
   zip_bool=False
   for dims in output.dims:
      if 'forecast_time' in dims or 'lv_' in dims:
         if output.dims[dims] == 1:
            output=output.sel({dims: output[dims].values[0]})
         else:
            dim_tracker[dims]=output[dims].values.tolist()
   #for forecast time selection I need to use np.timedelta64
   if len(dim_tracker)==0:
      f_location=self.dir_root+'/'+self.uuid+'.tif'
      output_array=output.to_array()
      output_array=output_array.compute()
      df=write_cog(output_array,fname=f_location)
   else:
      f_location=self.dir_root+'/temp_dir/'+self.uuid
      os.makedirs(f_location,exist_ok=True)
      if len(dim_tracker.keys())==2:
         for element1 in dim_tracker[list(dim_tracker.keys())[0]]:
            print(str(element1)+'-----')
            for element2 in dim_tracker[list(dim_tracker.keys())[1]]:
               sample=output.sel({list(dim_tracker.keys())[0]:np.timedelta64(element1),list(dim_tracker.keys())[1]:element2})
               sample_array=sample.to_array()
               sample_array=sample_array.compute()
               fname=str(element1)+'_'+str(int(element2))+'.tif'
               w_location=f_location+'/'+fname
               df=write_cog(sample_array,fname=w_location)
      if len(dim_tracker.keys())==1:
         if 'forecast_time' in list(dim_tracker.keys())[0]:
            for element in dim_tracker[list(dim_tracker.keys())[0]]:
               sample=output.sel({list(dim_tracker.keys())[0]:np.timedelta64(element)})
               sample_array=sample.to_array()
               sample_array=sample_array.compute()
               fname=str(element)+'.tif'
               w_location=f_location+'/'+fname
               df=write_cog(sample_array,fname=w_location)
         else:
            for element in dim_tracker[list(dim_tracker.keys())[0]]:
               sample=output.sel({list(dim_tracker.keys())[0]:element})
               sample_array=sample.to_array()
               sample_array=sample_array.compute()
               fname=str(element)+'.tif'
               w_location=f_location+'/'+fname
               df=write_cog(sample_array,fname=w_location)
      #create zip
      zip_bool=True
      base_path = pathlib.Path(f_location+'/')
      with zipfile.ZipFile(f_location+'.zip', mode='w') as z:
         for f_name in base_path.iterdir():
            z.write(f_name)
   return f_location, zip_bool
